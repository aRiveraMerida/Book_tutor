# üí∞ An√°lisis de Costes - BookTutor

Este documento analiza los costes asociados a la operaci√≥n de BookTutor, desglosando los gastos de infraestructura, procesamiento RAG y peticiones al tutor IA.

---

## üìä Resumen Ejecutivo

| Escenario | Coste Mensual | Coste por Petici√≥n |
|-----------|---------------|-------------------|
| **Self-hosted (Ollama local)** | ~‚Ç¨50-150 | ~‚Ç¨0.00 |
| **Cloud (OpenAI GPT-4o-mini)** | Variable | ~‚Ç¨0.002-0.01 |
| **H√≠brido (Cloud + Local)** | ~‚Ç¨100-300 | ~‚Ç¨0.001-0.005 |

---

## üèóÔ∏è Costes de Infraestructura

### Opci√≥n 1: Self-Hosted (Recomendado para Educaci√≥n)

#### Servidor sin GPU

| Proveedor | Especificaciones | Coste Mensual |
|-----------|-----------------|---------------|
| **Hetzner** | CPX41 (8 vCPU, 16GB RAM) | ~‚Ç¨30/mes |
| **DigitalOcean** | Premium (8 vCPU, 16GB) | ~‚Ç¨96/mes |
| **OVH** | B2-30 (8 vCPU, 30GB) | ~‚Ç¨50/mes |

> ‚ö†Ô∏è Sin GPU, cada petici√≥n al tutor tarda ~30-60 segundos con qwen3:8b

#### Servidor con GPU (Recomendado)

| Proveedor | GPU | Coste Mensual |
|-----------|-----|---------------|
| **Hetzner** | - | No disponible |
| **AWS** | g4dn.xlarge (T4) | ~‚Ç¨400/mes |
| **Lambda Labs** | A10 (24GB) | ~‚Ç¨300/mes |
| **Vast.ai** | RTX 4090 | ~‚Ç¨150-250/mes |

> ‚úÖ Con GPU, cada petici√≥n tarda ~2-5 segundos

### Opci√≥n 2: Servicios Cloud (Pago por Uso)

#### Qdrant Cloud

| Plan | Capacidad | Coste |
|------|-----------|-------|
| Free | 1GB, 1 nodo | ‚Ç¨0/mes |
| Starter | 4GB | ~‚Ç¨25/mes |
| Production | 20GB+ | ~‚Ç¨100+/mes |

Para BookTutor t√≠pico (10 asignaturas, 100 documentos): **~‚Ç¨0-25/mes**

---

## üß† Costes de Procesamiento RAG

El pipeline RAG tiene dos fases con costes diferentes:

### Fase 1: Indexaci√≥n (Embeddings)

Se ejecuta **una vez** al a√±adir/actualizar documentos.

#### Con Ollama (bge-m3) - GRATIS

```
Coste = ‚Ç¨0 (local)
Tiempo = ~0.5s por chunk
```

#### Con OpenAI (text-embedding-3-small)

```
Precio: $0.00002 / 1K tokens

Ejemplo para 1 asignatura:
- 10 documentos √ó 20 chunks √ó 300 tokens/chunk = 60,000 tokens
- Coste = 60,000 / 1,000 √ó $0.00002 = $0.0012 ‚âà ‚Ç¨0.001
```

**Coste t√≠pico de indexar todo el contenido**: ‚Ç¨0.01-0.10 (una vez)

### Fase 2: B√∫squeda (Query)

Se ejecuta en **cada pregunta** del usuario.

```
1 query embedding = ~20 tokens
Coste OpenAI = $0.00002 √ó 20 / 1000 = $0.0000004 ‚âà DESPRECIABLE
```

---

## ü§ñ Costes por Petici√≥n al Tutor

### Desglose de una Petici√≥n T√≠pica

```
Usuario pregunta: "¬øQu√© es Python?"

1. Embedding de la pregunta
   - Tokens: ~20
   - Coste (OpenAI): ~‚Ç¨0.0000004
   - Coste (Ollama): ‚Ç¨0

2. B√∫squeda en Qdrant
   - Operaciones: 1 query
   - Coste (Cloud): ~‚Ç¨0.000001
   - Coste (Self-hosted): ‚Ç¨0

3. Generaci√≥n de respuesta (LLM)
   - Input: ~2000 tokens (contexto + pregunta)
   - Output: ~500 tokens (respuesta)
   - Total: ~2500 tokens
```

### Comparativa de Modelos

| Modelo | Input (1M tok) | Output (1M tok) | Coste/Petici√≥n |
|--------|---------------|-----------------|----------------|
| **Ollama qwen3:8b** | ‚Ç¨0 | ‚Ç¨0 | **‚Ç¨0.00** |
| **OpenAI GPT-4o-mini** | $0.15 | $0.60 | **~‚Ç¨0.002** |
| **OpenAI GPT-4o** | $2.50 | $10.00 | **~‚Ç¨0.03** |
| **Claude 3.5 Haiku** | $0.25 | $1.25 | **~‚Ç¨0.004** |
| **Claude 3.5 Sonnet** | $3.00 | $15.00 | **~‚Ç¨0.05** |

### C√°lculo Detallado (GPT-4o-mini)

```
Input: 2000 tokens √ó $0.15/1M = $0.0003
Output: 500 tokens √ó $0.60/1M = $0.0003
Total por petici√≥n: $0.0006 ‚âà ‚Ç¨0.0005

+ Embedding: ~‚Ç¨0.0000004
+ Qdrant: ~‚Ç¨0.000001

TOTAL: ~‚Ç¨0.0005-0.001 por petici√≥n
```

---

## üìà Proyecciones de Uso

### Escenario: Centro Educativo Peque√±o

```
- 100 estudiantes
- 5 preguntas/estudiante/d√≠a
- 20 d√≠as lectivos/mes

Peticiones mensuales: 100 √ó 5 √ó 20 = 10,000 peticiones
```

| Opci√≥n | Coste LLM | Infra | **Total Mensual** |
|--------|-----------|-------|-------------------|
| Ollama (self-hosted) | ‚Ç¨0 | ‚Ç¨50-100 | **~‚Ç¨50-100** |
| GPT-4o-mini | ‚Ç¨5-10 | ‚Ç¨25 | **~‚Ç¨30-35** |
| GPT-4o | ‚Ç¨300 | ‚Ç¨25 | **~‚Ç¨325** |

### Escenario: Centro Educativo Grande

```
- 1000 estudiantes
- 10 preguntas/estudiante/d√≠a
- 20 d√≠as lectivos/mes

Peticiones mensuales: 1000 √ó 10 √ó 20 = 200,000 peticiones
```

| Opci√≥n | Coste LLM | Infra | **Total Mensual** |
|--------|-----------|-------|-------------------|
| Ollama (GPU) | ‚Ç¨0 | ‚Ç¨300-400 | **~‚Ç¨300-400** |
| GPT-4o-mini | ‚Ç¨100-200 | ‚Ç¨100 | **~‚Ç¨200-300** |
| GPT-4o | ‚Ç¨6000 | ‚Ç¨100 | **~‚Ç¨6100** |

---

## üí° Recomendaciones

### Para Desarrollo/Pruebas

```
‚úÖ Ollama local (qwen3:8b + bge-m3)
‚úÖ Qdrant local (Docker)
üí∞ Coste: ‚Ç¨0 (solo electricidad)
```

### Para Producci√≥n Peque√±a (<50 usuarios)

```
‚úÖ Servidor VPS sin GPU (Hetzner CPX41)
‚úÖ Ollama con modelo peque√±o (qwen3:4b)
‚úÖ Qdrant local
üí∞ Coste: ~‚Ç¨30-50/mes
‚è±Ô∏è Latencia: 20-40s por respuesta
```

### Para Producci√≥n Media (50-500 usuarios)

```
‚úÖ Servidor con GPU (Vast.ai RTX 4090)
‚úÖ Ollama qwen3:8b
‚úÖ Qdrant Cloud Starter
üí∞ Coste: ~‚Ç¨150-250/mes
‚è±Ô∏è Latencia: 2-5s por respuesta
```

### Para Producci√≥n Grande (>500 usuarios)

**Opci√≥n A: Full Cloud (Escalable)**
```
‚úÖ OpenAI GPT-4o-mini
‚úÖ Qdrant Cloud
‚úÖ Backend en Cloud Run/Lambda
üí∞ Coste: Variable (~‚Ç¨0.001/petici√≥n)
‚è±Ô∏è Latencia: 1-3s por respuesta
```

**Opci√≥n B: H√≠brido (Control + Escalabilidad)**
```
‚úÖ Servidor propio con GPU para carga base
‚úÖ OpenAI como fallback para picos
‚úÖ Qdrant Cloud para HA
üí∞ Coste: ~‚Ç¨300-500/mes base + variable
```

---

## üìâ Optimizaci√≥n de Costes

### 1. Reducir tokens de contexto

```python
# Actual: 6 chunks √ó 500 tokens = 3000 tokens
RETRIEVER_K = 6
CHUNK_SIZE = 500

# Optimizado: 4 chunks √ó 400 tokens = 1600 tokens
RETRIEVER_K = 4
CHUNK_SIZE = 400

# Ahorro: ~47% en costes de LLM
```

### 2. Cach√© de respuestas frecuentes

```python
# Implementar Redis para cachear preguntas comunes
# Ahorro estimado: 20-40% de peticiones
```

### 3. Rate limiting por usuario

```python
# Limitar a 20 preguntas/usuario/d√≠a
# Previene abuso y controla costes
```

### 4. Modelo m√°s peque√±o para preguntas simples

```python
# Clasificar complejidad de pregunta
# Simple ‚Üí qwen3:4b (m√°s r√°pido, menos recursos)
# Compleja ‚Üí qwen3:8b (m√°s preciso)
```

---

## üìä M√©tricas de Seguimiento

Para controlar costes, monitorizar:

```
- Peticiones/d√≠a por usuario
- Tokens promedio por petici√≥n
- Tiempo de respuesta (latencia)
- Tasa de cach√© hits
- Errores/timeouts
```

### Dashboard Recomendado

```bash
# Prometheus + Grafana
# M√©tricas a exportar:
- booktutor_requests_total
- booktutor_tokens_used
- booktutor_response_time_seconds
- booktutor_cache_hits_total
```

---

## üîÑ Actualizaci√≥n de Precios

Los precios de APIs de LLM cambian frecuentemente. √öltima actualizaci√≥n: **Febrero 2024**

Fuentes oficiales:
- [OpenAI Pricing](https://openai.com/pricing)
- [Anthropic Pricing](https://www.anthropic.com/pricing)
- [Qdrant Pricing](https://qdrant.tech/pricing/)

---

## üìù Conclusi√≥n

| Uso | Recomendaci√≥n | Coste Estimado |
|-----|---------------|----------------|
| Desarrollo | Ollama local | ‚Ç¨0 |
| Piloto (<50 usuarios) | VPS b√°sico + Ollama | ‚Ç¨30-50/mes |
| Producci√≥n media | GPU cloud + Ollama | ‚Ç¨150-300/mes |
| Producci√≥n grande | H√≠brido o full cloud | ‚Ç¨300-1000/mes |

**Para un centro educativo t√≠pico (100-500 estudiantes), el coste estimado es de ‚Ç¨100-300/mes**, significativamente menor que soluciones comerciales equivalentes.
