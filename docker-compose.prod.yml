# ================================
# BookTutor - Production
# ================================
# Usage: docker compose -f docker-compose.prod.yml up -d
#
# CI/CD: Este archivo se usa para despliegue automático.
# Cuando se hace push a main con cambios en backend/docs/,
# GitHub Actions construye y despliega automáticamente.

services:
  # ===================
  # Qdrant (Vector Store)
  # ===================
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: booktutor-qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
    networks:
      - booktutor

  # ===================
  # Ollama (LLM) - Opcional, puede correr en host
  # ===================
  ollama:
    image: ollama/ollama:latest
    container_name: booktutor-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: always
    deploy:
      resources:
        limits:
          memory: 8G
    networks:
      - booktutor
    # Después del primer inicio, descargar modelos:
    # docker exec booktutor-ollama ollama pull qwen3:4b
    # docker exec booktutor-ollama ollama pull bge-m3

  # ===================
  # Backend (FastAPI)
  # ===================
  backend:
    image: ghcr.io/${GITHUB_REPOSITORY:-booktutor}/backend:${VERSION:-latest}
    build:
      context: .
      dockerfile: docker/Dockerfile.backend
    container_name: booktutor-backend
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=prod
      - DEBUG=false
      - QDRANT_URL=http://qdrant:6333
      - OLLAMA_BASE_URL=http://ollama:11434
      - DOCS_DIR=/app/docs
      # Configuración optimizada para bajo coste
      - DEFAULT_LLM_MODEL=qwen3:4b
      - LLM_MAX_TOKENS=2048
      - LLM_TEMPERATURE=0.2
      - CHUNK_SIZE=1000
      - RETRIEVER_K=4
      - CORS_ORIGINS=["http://localhost:3000","https://${DOMAIN:-localhost}"]
    volumes:
      - ./backend/docs:/app/docs:ro
    depends_on:
      qdrant:
        condition: service_healthy
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M
    networks:
      - booktutor
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===================
  # Frontend (Next.js)
  # ===================
  frontend:
    image: ghcr.io/${GITHUB_REPOSITORY:-booktutor}/frontend:${VERSION:-latest}
    build:
      context: .
      dockerfile: docker/Dockerfile.frontend
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000/api/v1}
    container_name: booktutor-frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend
    restart: always
    deploy:
      resources:
        limits:
          memory: 256M
    networks:
      - booktutor

volumes:
  qdrant_data:
    name: booktutor-qdrant-data
  ollama_data:
    name: booktutor-ollama-data

networks:
  booktutor:
    name: booktutor-network
    driver: bridge
