# ğŸ“š BookTutor

**Plataforma educativa con tutor IA basado en RAG (Retrieval-Augmented Generation)**

BookTutor permite a los estudiantes leer documentos de estudio y hacer preguntas a un tutor IA que responde basÃ¡ndose exclusivamente en el contenido del material.

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Frontend     â”‚â”€â”€â”€â”€â–¶â”‚     Backend     â”‚â”€â”€â”€â”€â–¶â”‚     Ollama      â”‚
â”‚   (Next.js)     â”‚     â”‚   (FastAPI)     â”‚     â”‚   (LLM Local)   â”‚
â”‚   Puerto 3000   â”‚     â”‚   Puerto 8000   â”‚     â”‚  Puerto 11434   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚     Qdrant      â”‚
                        â”‚ (Vector Store)  â”‚
                        â”‚   Puerto 6333   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stack TecnolÃ³gico

| Componente | TecnologÃ­a | VersiÃ³n |
|------------|------------|---------|
| Frontend | Next.js + React | 15.x |
| Backend | FastAPI + Python | 3.12+ |
| LLM | Ollama (qwen3:4b) | Local |
| Embeddings | Ollama (bge-m3) | 1024 dims |
| Vector DB | Qdrant | 1.7.x |

---

## ğŸ“ Estructura del Proyecto

```
Book_tutor/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/v1/           # Endpoints REST
â”‚   â”‚   â”‚   â”œâ”€â”€ asignaturas.py # Listar asignaturas
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py       # RAG Q&A
â”‚   â”‚   â”‚   â””â”€â”€ health.py     # Health check
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â””â”€â”€ config.py     # Settings (pydantic)
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â””â”€â”€ qdrant.py     # Qdrant client
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â””â”€â”€ ollama.py     # Ollama provider
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ auto_ingest.py    # Auto-RAG on startup
â”‚   â”‚       â”œâ”€â”€ ingest_service.py # Document processing
â”‚   â”‚       â””â”€â”€ rag_service.py    # RAG orchestration
â”‚   â”œâ”€â”€ docs/                  # ğŸ“‚ Documentos por asignatura
â”‚   â”‚   â”œâ”€â”€ programacion/      # â†’ Auto-RAG como "programacion"
â”‚   â”‚   â””â”€â”€ bases-de-datos/    # â†’ Auto-RAG como "bases-de-datos"
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/                   # Next.js App Router
â”‚   â”‚   â”œâ”€â”€ asignatura/[subject]/
â”‚   â”‚   â””â”€â”€ api/subjects/      # API routes
â”‚   â””â”€â”€ components/
â”‚       â””â”€â”€ chat/              # Chat components
â”‚
â”œâ”€â”€ .github/workflows/         # CI/CD
â”‚   â”œâ”€â”€ ci.yml                 # Tests en PRs
â”‚   â””â”€â”€ deploy.yml             # Deploy automÃ¡tico
â”‚
â”œâ”€â”€ docker-compose.yml         # Desarrollo
â””â”€â”€ docker-compose.prod.yml    # ProducciÃ³n
```

---

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos

- Python 3.12+
- Node.js 20+
- Docker (para Qdrant)
- Ollama instalado localmente

### 1. Clonar y configurar

```bash
git clone <repo-url>
cd Book_tutor
cp backend/.env.example backend/.env
```

### 2. Iniciar Qdrant

```bash
docker compose up -d qdrant
```

### 3. Configurar Ollama

```bash
# Instalar modelos (optimizado para bajo coste)
ollama pull qwen3:4b      # Chat model (~2.5GB)
ollama pull bge-m3        # Embeddings (~1.5GB)
```

### 4. Backend

```bash
cd backend
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Iniciar servidor (auto-ingesta RAGs de docs/)
uvicorn app.main:app --reload --port 8000
```

### 5. Frontend

```bash
cd frontend
npm install
npm run dev
```

### 6. Acceder

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000/docs
- **Qdrant UI**: http://localhost:6333/dashboard

---

## ğŸ“– Uso del Sistema

### Flujo AutomÃ¡tico

1. **Subir documentos** â†’ Crear carpeta en `backend/docs/{asignatura}/` con archivos `.md`
2. **Reiniciar backend** â†’ Auto-ingesta detecta nueva carpeta y crea RAG
3. **Chatbot disponible** â†’ Los estudiantes pueden hacer preguntas sobre la asignatura

### AÃ±adir Nueva Asignatura

```bash
# 1. Crear carpeta con nombre de la asignatura (slug)
mkdir backend/docs/matematicas

# 2. AÃ±adir documentos markdown
cp mis-apuntes/*.md backend/docs/matematicas/

# 3. Reiniciar backend (auto-ingesta)
# El chatbot de "matematicas" estarÃ¡ disponible automÃ¡ticamente
```

---

## ğŸ”Œ API Endpoints

### Asignaturas

```
GET  /api/v1/asignaturas              # Listar todas
GET  /api/v1/asignaturas/{slug}       # Detalle + documentos
GET  /api/v1/asignaturas/{slug}/documents/{file}  # Contenido documento
```

### Chat RAG

```
POST /api/v1/chat/{slug}/ask
Body: { "question": "Â¿QuÃ© es Python?" }
Response: {
  "answer": "Python es un lenguaje...",
  "sources": [
    { "source_file": "01-intro.md", "seccion": "IntroducciÃ³n", "score": 0.85 }
  ],
  "model_used": "qwen3:4b"
}

POST /api/v1/chat/{slug}/stream  # Respuesta en streaming (SSE)
```

### Health Check

```
GET /api/v1/health
Response: { "status": "ok", "ollama": { "status": "ok", "models": [...] } }
```

---

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

```env
# Ollama
OLLAMA_BASE_URL=http://localhost:11434

# Modelos (optimizado para bajo coste)
DEFAULT_LLM_MODEL=qwen3:4b
EMBEDDING_MODEL=bge-m3

# Qdrant
QDRANT_URL=http://localhost:6333

# RAG Settings (optimizado)
CHUNK_SIZE=1000
RETRIEVER_K=4
LLM_MAX_TOKENS=2048

# Documents
DOCS_DIR=./docs
```

---

## ğŸš€ CI/CD

El proyecto usa GitHub Actions para despliegue automÃ¡tico:

1. **Push a `main`** con cambios en `backend/docs/`
2. **GitHub Actions** construye imÃ¡genes Docker
3. **Deploy automÃ¡tico** al servidor
4. **Backend reinicia** y auto-ingesta nuevas asignaturas
5. **Chatbot disponible** automÃ¡ticamente

### Configurar Secretos en GitHub

```
SERVER_HOST      â†’ IP del servidor
SERVER_USER      â†’ Usuario SSH
SERVER_SSH_KEY   â†’ Clave privada SSH
DEPLOY_PATH      â†’ Ruta del proyecto en servidor
```

---

## ğŸ“š DocumentaciÃ³n Adicional

- [DEPLOY.md](./DEPLOY.md) - GuÃ­a de despliegue en producciÃ³n
- [COST.md](./COST.md) - AnÃ¡lisis de costes

---

## ğŸ“œ Licencia

MIT License

---

## ğŸ‘¥ Equipo

Desarrollado por el equipo de FP Prometeo.
